{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: CESNET + Ucdavis-icdm19\n",
    "#### Author: Jakub Čoček (xcocek00)\n",
    "\n",
    "| Augmentations | Filters | Model | Validation | Fine-tuning | Testing |\n",
    "|---------------|---------|----------|------------|----------------|---------|\n",
    "| Change in Inter-Arrival Time (IAT) | none | trained on CESNET | n/a | Ucdavis - train | Ucdavis - script |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IMPORTS --\n",
    "\n",
    "import os\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torchdata.datapipes.iter import FileLister\n",
    "\n",
    "# flowmind imports\n",
    "os.chdir('/workplace/flowmind/')\n",
    "from flowmind.contrastive import NTXentLoss\n",
    "\n",
    "# livelossplot imports\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "\n",
    "# others\n",
    "from functools import partial\n",
    "\n",
    "# common imports\n",
    "os.chdir('/workplace/xcocek00/common/')\n",
    "from dataloader import create_flowpic_dataloader\n",
    "\n",
    "os.chdir('/workplace/xcocek00/common/')\n",
    "from nn import CNN, MLP\n",
    "\n",
    "# sets csv limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# sets the device to use gpu if available, if not, use cpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- custom remap\n",
    "\n",
    "def remap_label(t: tuple[torch.Tensor, torch.Tensor, str], labels_d: dict[str, int]) -> tuple[torch.Tensor, torch.Tensor, int]:\n",
    "    return t[0], t[1], labels_d[t[2]]\n",
    "\n",
    "labels_ucdavis = [\n",
    "    \"google-doc\",\n",
    "    \"google-drive\",\n",
    "    \"google-music\",\n",
    "    \"google-search\",\n",
    "    \"youtube\",\n",
    "]\n",
    "\n",
    "labels_ucdavis_d = {label: i for i, label in enumerate(labels_ucdavis)}\n",
    "\n",
    "remap_label_ucdavis = partial(remap_label, labels_d=labels_ucdavis_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADERS --\n",
    "# dataloaders yield tuple (flowpic1, flowpic2, label)\n",
    "# using only 15 seconds of the flow\n",
    "\n",
    "# dataloader used for fine-tuning\n",
    "dl_fine_t = create_flowpic_dataloader(\n",
    "    dir_path=\"/workplace/datasets/ucdavis/final-splits/train.csv\",    \n",
    "    batch_size=32,\n",
    "    meta_key=\"app\",\n",
    "    time_bins = [i * (15 / 32) for i in range(33)],\n",
    "    length_bins = [i * (1500 / 32) for i in range(33)],\n",
    "    bidirectional = False,\n",
    "    dp_transform=lambda dp: dp.map(remap_label_ucdavis).in_memory_cache(),\n",
    ")\n",
    "\n",
    "# dataloader used for testing\n",
    "dl_test = create_flowpic_dataloader(\n",
    "    dir_path=\"/workplace/datasets/ucdavis/final-splits/script.csv\",\n",
    "    batch_size=32,\n",
    "    meta_key=\"app\",\n",
    "    time_bins = [i * (15 / 32) for i in range(33)],\n",
    "    length_bins = [i * (1500 / 32) for i in range(33)],\n",
    "    bidirectional = False,\n",
    "    dp_transform=lambda dp: dp.map(remap_label_ucdavis).in_memory_cache(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- training loops--\n",
    "\n",
    "def classification(CNN_model, MLP_model, dataloader, log, loss_fn, accuracy, optimizer):\n",
    "    '''\n",
    "    Classification loop\n",
    "\n",
    "    Args:\n",
    "        CNN_model: CNN model \n",
    "        MLP_model: MLP model\n",
    "        dataloader: pytorch dataloader created by create_flowpic_dataloader function\n",
    "        log: for log collection\n",
    "        loss_fn: chosen loss function\n",
    "        accuracy: chosen model accuracy\n",
    "        optimizer: chosen optimizer\n",
    "\n",
    "    Returns:\n",
    "        classification loss \n",
    "    '''\n",
    "\n",
    "    # freeze the encoder\n",
    "    for param in CNN_model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # remove projection head\n",
    "    CNN_model.projection = nn.Sequential()\n",
    "    \n",
    "    # set models\n",
    "    CNN_model.eval()\n",
    "    MLP_model.train()\n",
    "\n",
    "    classification_loss = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    for flowpic, _, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        flowpic, label = flowpic.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = CNN_model.encoder(flowpic)\n",
    "        \n",
    "        y_pred = MLP_model(embeddings)\n",
    "\n",
    "        loss = loss_fn(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        classification_loss += loss.item()\n",
    "        batches += 1\n",
    "        \n",
    "        accuracy.update(y_pred, label)\n",
    "    \n",
    "    classification_loss /= batches\n",
    "\n",
    "    log[\"classification_loss\"] = classification_loss\n",
    "    log[\"accuracy\"] = accuracy.compute()\n",
    "        \n",
    "    accuracy.reset()\n",
    "\n",
    "    return classification_loss\n",
    "\n",
    "def testing(CNN_model, MLP_model, dataloader, log, loss_fn, accuracy):\n",
    "    '''\n",
    "    Testing loop\n",
    "\n",
    "    Args:\n",
    "        CNN_model: CNN model \n",
    "        MLP_model: MLP model\n",
    "        dataloader: pytorch dataloader created by create_flowpic_dataloader function\n",
    "        log: for log collection\n",
    "        loss_fn: chosen loss function\n",
    "        accuracy: chosen model accuracy\n",
    "        optimizer: chosen optimizer\n",
    "\n",
    "    Returns:\n",
    "        training loss \n",
    "    '''\n",
    "\n",
    "    # freeze the encoder\n",
    "    for param in CNN_model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # set models\n",
    "    CNN_model.eval()\n",
    "    MLP_model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for flowpic_t, _, label in dataloader:\n",
    "            flowpic_t, label = flowpic_t.to(device), label.to(device)\n",
    "\n",
    "            embeddings = CNN_model.encoder(flowpic_t)\n",
    "            y_pred = MLP_model(embeddings)\n",
    "\n",
    "            loss = loss_fn(y_pred, label)\n",
    "            test_loss += loss.item()\n",
    "            batches += 1\n",
    "\n",
    "            accuracy.update(y_pred, label)\n",
    "    \n",
    "        test_loss /= batches\n",
    "    \n",
    "        log[\"model_loss\"] = test_loss\n",
    "        log[\"model_accuracy\"] = accuracy.compute()\n",
    "            \n",
    "        accuracy.reset()\n",
    "\n",
    "    return test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- training linear classifier --\n",
    "\n",
    "def train_classifier(cnn_model, mlp_model, dl_class, mlp_loss_fn, mlp_accuracy, mlp_optimizer, dl_test, mlp_liveloss):\n",
    "    # set params for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    min_improvement = 0.001\n",
    "    patience = 5\n",
    "    best_acc = float('-inf')\n",
    "    f_acc = 0\n",
    "\n",
    "    for epoch in range(20):\n",
    "        log = {}\n",
    "        classification_loss = classification(cnn_model, mlp_model, dl_class, log, mlp_loss_fn, mlp_accuracy, mlp_optimizer)   \n",
    "        test = testing(cnn_model, mlp_model, dl_test, log, mlp_loss_fn, mlp_accuracy)   \n",
    "\n",
    "        mlp_liveloss.update(log)\n",
    "        mlp_liveloss.send()\n",
    "\n",
    "        if best_acc < log['model_accuracy']:\n",
    "            best_acc = log['model_accuracy']\n",
    "\n",
    "        # early stopping\n",
    "        if best_loss - classification_loss > min_improvement:\n",
    "            best_loss = classification_loss\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(\"Stopping ... epoch \", epoch)\n",
    "            break\n",
    "            \n",
    "    f_acc = log['model_accuracy']\n",
    "    return f_acc, best_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 5\n",
    "\n",
    "# loss functions\n",
    "contrastive_loss_fn = NTXentLoss(temperature=0.07)\n",
    "mlp_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "final_acc_runs = []\n",
    "best_acc_runs = []\n",
    "for i in range(5):\n",
    "    # load CNN model\n",
    "    cnn_model = CNN().to(device)\n",
    "    cnn_model.load_state_dict(torch.load(\"/workplace/xcocek00/models/cnn_model.pth\"))\n",
    "\n",
    "    # Linear classifier\n",
    "    mlp_model = MLP(classes).to(device)\n",
    "    mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.01)\n",
    "    mlp_liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6, 2))])\n",
    "    mlp_accuracy = MulticlassAccuracy()\n",
    "    \n",
    "    # training loops\n",
    "    f_acc, b_acc = train_classifier(cnn_model, mlp_model, dl_fine_t, mlp_loss_fn, mlp_accuracy, mlp_optimizer, dl_test, mlp_liveloss)\n",
    "\n",
    "    final_acc_runs.append(f_acc)\n",
    "    best_acc_runs.append(b_acc)\n",
    "\n",
    "    # print accuracy\n",
    "    print(f\"Run {i}: final accuracy: {f_acc}\")\n",
    "    print(f\"Run {i}: best accuracy: {b_acc}\")\n",
    "\n",
    "# calculate mean\n",
    "f_acc_p = torch.stack(final_acc_runs)\n",
    "b_acc_p = torch.stack(best_acc_runs)\n",
    "\n",
    "mean_f_acc = f_acc_p.mean().item()\n",
    "std_f_acc = f_acc_p.std(unbiased=True).item()\n",
    "\n",
    "mean_b_acc = b_acc_p.mean().item()\n",
    "std_b_acc = b_acc_p.std(unbiased=True).item()\n",
    "\n",
    "print(f\"Final accuracy: {mean_f_acc:.4f} ± {std_f_acc:.4f}\")\n",
    "print(f\"Best accuracy: {mean_b_acc:.4f} ± {std_b_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
