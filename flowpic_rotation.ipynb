{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IMPORTS --\n",
    "'''\n",
    "%pip install livelossplot\n",
    "%pip install torcheval\n",
    "%pip install torchmetrics\n",
    "'''\n",
    "import os\n",
    "os.chdir('/workplace/flowmind/')\n",
    "print(\"working in: \" + os.getcwd())\n",
    "\n",
    "#%run /workplace/flowmind/setup.py\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as FM\n",
    "#import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "# flowmind imports\n",
    "#from flowmind.processing.dataloaders.flowpic import create_flowpic_dataloader \n",
    "from flowmind.datasets.mirage import remap_label_mirage19\n",
    "from flowmind.processing.dataloaders.common import FlowData\n",
    "\n",
    "# livelossplot imports\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "\n",
    "# others\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "os.chdir('/workplace/xcocek00/')\n",
    "print(\"working in: \" + os.getcwd())\n",
    "from flowpic import create_flowpic_dataloader\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# sets the device to use gpu if available, if not, use cpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TESTS --\n",
    "\n",
    "# dimensions validation\n",
    "def dim_val(dl_train) -> None:\n",
    "    print(\"dl train: \", type(dl_train))\n",
    "    for (flowpic1, flowpic2, labels) in dl_train:\n",
    "        print(\"flowpic1\", type(flowpic1))\n",
    "        print(\"flowpic2\", type(flowpic2))\n",
    "        print(\"flowpic1 shape: \", flowpic1.shape)\n",
    "        print(\"flowpic2 shape: \", flowpic2.shape)\n",
    "        break    \n",
    "\n",
    "# print batch\n",
    "def print_batch(dl_train) -> None:\n",
    "    for batch in dl_train:\n",
    "        torch.set_printoptions(threshold=sys.maxsize)\n",
    "        print(batch)\n",
    "        break\n",
    "    \n",
    "# debug batch    \n",
    "def debug_batch(dl):\n",
    "    for (flowpic1, flowpic2, labels) in dl:\n",
    "        print(\"flowpic1 type:\", type(flowpic1))\n",
    "        print(\"flowpic2 type:\", type(flowpic2))\n",
    "        # If they are lists, let's check each item\n",
    "        if isinstance(flowpic1, list):\n",
    "            print(f\"flowpic1 has {len(flowpic1)} items.\")\n",
    "            print(\"Shapes of flowpic1 items:\")\n",
    "            for i, fp in enumerate(flowpic1):\n",
    "                print(type(fp))\n",
    "                print(f\"  Item {i}: {fp.shape}\")\n",
    "                break\n",
    "\n",
    "        if isinstance(flowpic2, list):\n",
    "            print(f\"flowpic2 has {len(flowpic2)} items.\")\n",
    "            print(\"Shapes of flowpic2 items:\")\n",
    "            for i, fp in enumerate(flowpic2):\n",
    "                print(f\"  Item {i}: {fp.shape}\")\n",
    "\n",
    "        print(\"labels:\", labels)\n",
    "        break  # Just inspect the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FlowPic rotation --\n",
    "\n",
    "def flowpic_rotation(flowpic):\n",
    "    '''\n",
    "    FlowPic rotation augmentation - rotating flowpic within -10° and 10°\n",
    "\n",
    "    Args:\n",
    "        flowpic: original flowpic\n",
    "\n",
    "    Returns:\n",
    "        rotated flowpic\n",
    "    '''\n",
    "    rotate_transform = T.RandomRotation(degrees=(-10,10))\n",
    "    return rotate_transform(flowpic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER --\n",
    "\n",
    "# 0 - debug prints are turned off\n",
    "# 1 - debug prints are turned ofn\n",
    "DEBUG = 0\n",
    "\n",
    "dl_train = create_flowpic_dataloader(\n",
    "    #dir_path=\"/workplace/xcocek00/test_b.csv\",\n",
    "    dir_path=\"/workplace/datasets/mirage19/processed/splits/train.csv\",\n",
    "    batch_size=32,\n",
    "    meta_key=\"BF_label\",\n",
    "    time_bins = [i * (300 / 32) for i in range(33)],\n",
    "    length_bins = [i * (1500 / 32) for i in range(33)],\n",
    "    # dp_transform=lambda dp: dp.map(remap_label_mirage19).in_memory_cache(),\n",
    ")\n",
    "dl_val = create_flowpic_dataloader(\n",
    "    dir_path=\"/workplace/datasets/mirage19/processed/splits/val.csv\",\n",
    "    batch_size=32,\n",
    "    meta_key=\"BF_label\",\n",
    "    time_bins = [i * (300 / 32) for i in range(33)],\n",
    "    length_bins = [i * (1500 / 32) for i in range(33)],\n",
    "    # dp_transform=lambda dp: dp.map(remap_label_mirage19).in_memory_cache(),\n",
    ")    \n",
    "\n",
    "\n",
    "if DEBUG:    \n",
    "    #print(type(dl_train))\n",
    "    dim_val(dl_train)\n",
    "    print_batch(dl_train)\n",
    "    #debug_batch(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- simple MLP --\n",
    "# FIXME: DELETE\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, channels, height, width, embed_dim=120):\n",
    "        super().__init__()\n",
    "        input_size = channels * height * width\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, height, width]\n",
    "        x = x.flatten(start_dim=1)  # -> [batch_size, channels*height*width]\n",
    "        z = self.encoder(x)         # -> [batch_size, embed_dim]\n",
    "        z = F.normalize(z, dim=1)   # L2 normalize\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CNN architecture --\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # conv + pooling layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "\n",
    "        # fully connected (fc) layers\n",
    "        self.fc1 = nn.Linear(400, 120) # 16*5*5 = 400 \n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        self.fc3 = nn.Linear(120, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv + ReLU + pool 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # conv + ReLU + pool 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # flatten for fc layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # end of encoder f(·) -> h\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # projection head g(·) -> z\n",
    "        z = F.relu(self.fc2(h))   \n",
    "        z = self.fc3(z)           \n",
    "        \n",
    "        # L2 normalize\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        '''\n",
    "        Returns:\n",
    "          h: 120-d representation\n",
    "          z: 30-d projection'\n",
    "        '''\n",
    "        return h, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TEST accuracy calculation --\n",
    "\n",
    "def contrastive_accuracy(z1, z2, temperature=0.07):\n",
    "    '''\n",
    "    Computes contrastive accuracy using a multiclass approach.\n",
    "    \n",
    "    Args:\n",
    "        z1: tensor \n",
    "        z2: tensor \n",
    "        temperature: scaling factor\n",
    "    \n",
    "    '''\n",
    "    batch_size = z1.size(0)\n",
    "    embeddings = torch.cat([z1, z2], dim=0)\n",
    "    \n",
    "    # pairwise cosine similarity scaled by temperature\n",
    "    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "\n",
    "    # mask self-similarity\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=sim_matrix.device)\n",
    "    sim_matrix.masked_fill_(mask, -float(\"inf\"))\n",
    "    \n",
    "    # predictions\n",
    "    preds = sim_matrix.argmax(dim=1)\n",
    "    \n",
    "    # targets\n",
    "    targets = (torch.arange(2 * batch_size, device=sim_matrix.device) + batch_size) % (2 * batch_size)\n",
    "    \n",
    "    acc = FM.accuracy(preds, targets, task=\"multiclass\", num_classes=2 * batch_size)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- training --\n",
    "\n",
    "def train(model, dataloader, optimizer, contrastive_loss_fn, log):\n",
    "    '''\n",
    "    Training loop\n",
    "\n",
    "    Args:\n",
    "        model: chosen model for training\n",
    "        dataloader: pytorch dataloader created by create_flowpic_dataloader function\n",
    "        optimizer: chosen optimizer\n",
    "        contrastive_loss_fn: contrastive loss function (NT-Xent)\n",
    "        log: for log collection\n",
    "\n",
    "    Returns:\n",
    "        Avarage loss \n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batches = 0\n",
    "    acc_total = 0.0\n",
    "\n",
    "    # label not needed in -> _\n",
    "    for flowpic1, flowpic2, _ in dataloader:\n",
    "       \n",
    "        # apply rotation augmentation\n",
    "        flowpic1 = flowpic_rotation(flowpic1)\n",
    "        flowpic2 = flowpic_rotation(flowpic2)\n",
    "        \n",
    "        flowpic1, flowpic2 = flowpic1.to(device), flowpic2.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass to get embeddings\n",
    "        h1, z1 = model(flowpic1)  \n",
    "        h2, z2 = model(flowpic2)  \n",
    "\n",
    "        # contrastive loss\n",
    "        loss = contrastive_loss_fn(z1, z2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "        \n",
    "        batch_acc = contrastive_accuracy(z1, z2, temperature=0.07)\n",
    "        acc_total += batch_acc.item()\n",
    "\n",
    "    avg_loss = total_loss / batches\n",
    "    avg_acc = acc_total / batches\n",
    "    log[\"loss\"] = avg_loss\n",
    "    log[\"accuracy\"] = avg_acc\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def val(model, dataloader, contrastive_loss_fn, log):\n",
    "    '''\n",
    "    Validation loop\n",
    "\n",
    "    Args:\n",
    "        model: chosen model for training\n",
    "        dataloader: pytorch dataloader created by create_flowpic_dataloader function\n",
    "        contrastive_loss_fn: contrastive loss function (NT-Xent)\n",
    "        log: for log collection\n",
    "\n",
    "    Returns:\n",
    "        Avarage loss \n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        batches = 0\n",
    "        acc_total = 0.0\n",
    "\n",
    "        # label not needed in -> _\n",
    "        for flowpic1, flowpic2, _ in dataloader:\n",
    "            \n",
    "            # apply rotation augmentation\n",
    "            flowpic1 = flowpic_rotation(flowpic1)\n",
    "            flowpic2 = flowpic_rotation(flowpic2)\n",
    "\n",
    "            flowpic1, flowpic2 = flowpic1.to(device), flowpic2.to(device)\n",
    "\n",
    "            # forward pass to get embeddings\n",
    "            h1, z1 = model(flowpic1)  \n",
    "            h2, z2 = model(flowpic2)  \n",
    "\n",
    "            # contrastive loss\n",
    "            loss = contrastive_loss_fn(z1, z2)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batches += 1\n",
    "            \n",
    "            batch_acc = contrastive_accuracy(z1, z2, temperature=0.07)\n",
    "            acc_total += batch_acc.item()\n",
    "\n",
    "        avg_loss = total_loss / batches\n",
    "        avg_acc = acc_total / batches\n",
    "        log[\"val_loss\"] = avg_loss\n",
    "        log[\"val_accuracy\"] = avg_acc\n",
    "\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- loss function --\n",
    "\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    '''\n",
    "    NT-Xent (Normalized Temperature-Scaled Cross-Entropy) loss function\n",
    "    \n",
    "    Args:\n",
    "        z1: tensor \n",
    "        z2: tensor \n",
    "        temperature: scaling factor\n",
    "    \n",
    "    Returns:\n",
    "        A scalar loss value\n",
    "    '''\n",
    "    \n",
    "    batch_size = z1.shape[0]\n",
    "    \n",
    "    # concatenate embeddings -> shape [2N, embed_dim]\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    \n",
    "    # cosine similarity matrix scaled by temperature\n",
    "    sim_matrix = torch.matmul(z, z.T) / temperature\n",
    "    \n",
    "    # mask to remove self-similarities (diagonal entries)\n",
    "    diag_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "    sim_matrix.masked_fill_(diag_mask, -float('inf'))\n",
    "    \n",
    "    # for each i the positive example is:\n",
    "    # - i in [0, N-1] then positive is: i + N\n",
    "    # - i in [N, 2N-1] then positive is: i - N\n",
    "    positives = torch.cat([\n",
    "        torch.arange(batch_size, 2 * batch_size),\n",
    "        torch.arange(0, batch_size)\n",
    "    ], dim=0).to(z.device)\n",
    "    \n",
    "    # cross entropy loss\n",
    "    loss = F.cross_entropy(sim_matrix, positives)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirage19_classes = 20\n",
    "\n",
    "# model\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# NT-Xent loss function\n",
    "loss_fn = nt_xent_loss\n",
    "\n",
    "liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6, 2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    log = {}\n",
    "    train_loss = train(model, dl_train, optimizer, loss_fn, log)\n",
    "    val_loss = val(model, dl_val, loss_fn, log)\n",
    "\n",
    "    liveloss.update(log)\n",
    "    liveloss.send()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
