{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IMPORTS --\n",
    "'''\n",
    "%pip install livelossplot\n",
    "%pip install torcheval\n",
    "%pip install torchmetrics\n",
    "'''\n",
    "import os\n",
    "os.chdir('/workplace/flowmind/')\n",
    "print(\"working in: \" + os.getcwd())\n",
    "\n",
    "#%run /workplace/flowmind/setup.py\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as FM\n",
    "#import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "# flowmind imports\n",
    "#from flowmind.processing.dataloaders.flowpic import create_flowpic_dataloader \n",
    "from flowmind.datasets.mirage import remap_label_mirage19\n",
    "from flowmind.processing.dataloaders.common import FlowData\n",
    "\n",
    "# livelossplot imports\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "\n",
    "# others\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "os.chdir('/workplace/xcocek00/')\n",
    "print(\"working in: \" + os.getcwd())\n",
    "from flowpic import create_flowpic_dataloader\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# sets the device to use gpu if available, if not, use cpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- RTT augmentation --\n",
    "\n",
    "def augment_rtt(flow: FlowData, alpha_min: float = 0.5, alpha_max: float = 1.5) -> FlowData:\n",
    "    \"\"\"\n",
    "    Multiply arrival time of each packet by a factor alpha, where\n",
    "    alpha is chosen uniformly in [alpha_min, alpha_max]\n",
    "\n",
    "    Args\n",
    "        flow: original flow\n",
    "        alpha_min: min factor set to 0.5\n",
    "        alpha_max: max factor set to 1.5\n",
    "\n",
    "    Returns: \n",
    "        modified FlowData\n",
    "    \"\"\"\n",
    "    \n",
    "    if not flow.times:\n",
    "        return flow\n",
    "\n",
    "    # select random alpha \n",
    "    alpha = random.uniform(alpha_min, alpha_max)\n",
    "    init_time = flow.init_time\n",
    "\n",
    "    # convert time to float offset\n",
    "    offsets = [(t - init_time).total_seconds() for t in flow.times]\n",
    "    \n",
    "    # RTT augmentation\n",
    "    offsets = [offset * alpha for offset in offsets]\n",
    "\n",
    "    # convert back\n",
    "    flow.times = [init_time + timedelta(seconds=offset) for offset in offsets]\n",
    "\n",
    "    return flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IAT augmentation --\n",
    "\n",
    "def augment_iat(flow: FlowData, b_min: float = -1.0, b_max: float = 1.0) -> FlowData:\n",
    "    '''\n",
    "    Add factor b to the arrival time of each packet, where\n",
    "    b is chosen uniformly in [b_min, b_max]\n",
    "\n",
    "    Args:\n",
    "        flow: original flow\n",
    "        b_min: min factor set to -1.0 \n",
    "        b_max: max factor set to 1.0\n",
    "\n",
    "    Returns: \n",
    "        modified FlowData\n",
    "    '''\n",
    "    \n",
    "    if not flow.times:\n",
    "        return flow\n",
    "\n",
    "    # select random b \n",
    "    b = random.uniform(b_min, b_max)\n",
    "    init_time = flow.init_time\n",
    "\n",
    "    # convert time to float offset\n",
    "    offsets = [(t - init_time).total_seconds() for t in flow.times]\n",
    "    \n",
    "    # IAT augmentation\n",
    "    offsets = [offset + b for offset in offsets]\n",
    "\n",
    "    # convert back\n",
    "    flow.times = [init_time + timedelta(seconds=offset) for offset in offsets]\n",
    "\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- packet loss augmentation --\n",
    "\n",
    "def packet_loss(flow: FlowData, dt: float = 0.1) -> FlowData:\n",
    "    '''\n",
    "    Packet loss augmentation - removing all packets within [t - dt, t + dt]\n",
    "    \n",
    "    Args:\n",
    "        flow: original flow\n",
    "        dt: delta t (interval length) set to 0.1\n",
    "    \n",
    "    Returns: \n",
    "        modified Flow with removed packets in that interval\n",
    "    ''' \n",
    "\n",
    "    if not flow.times:\n",
    "        return flow\n",
    "    \n",
    "    init_time = flow.init_time \n",
    "    \n",
    "    # conver to offset\n",
    "    offsets = [(t - init_time).total_seconds() for t in flow.times]\n",
    "    flow_duration = offsets[-1] if offsets else 0.0\n",
    "\n",
    "    # zero or near-zero duration -> skip (would remove the whole flow)\n",
    "    # TODO: consider 2*dt\n",
    "    if flow_duration <= 0:\n",
    "        return flow\n",
    "\n",
    " \n",
    "    t = random.uniform(0, flow_duration)\n",
    "\n",
    "    # interval [t - dt, t + dt]\n",
    "    lower = t - dt\n",
    "    upper = t + dt\n",
    "\n",
    "    # lists for filtered packets\n",
    "    new_directions = []\n",
    "    new_lengths    = []\n",
    "    new_times      = []\n",
    "    new_push_flags = []\n",
    "\n",
    "    for offset, direction, length, time_val, push_flag in zip(offsets, flow.directions, flow.lengths, flow.times, flow.push_flags):\n",
    "        if not (lower <= offset <= upper):\n",
    "            new_directions.append(direction)\n",
    "            new_lengths.append(length)\n",
    "            new_times.append(time_val)\n",
    "            new_push_flags.append(push_flag)\n",
    "\n",
    "    # replace old lists with the fitered lists\n",
    "    flow.directions = new_directions\n",
    "    flow.lengths    = new_lengths\n",
    "    flow.times      = new_times\n",
    "    flow.push_flags = new_push_flags\n",
    "    flow.init_time  = flow.times[0] if flow.times else None\n",
    "\n",
    "    return flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TESTS --\n",
    "\n",
    "# dimensions validation\n",
    "def dim_val(dl_train) -> None:\n",
    "    print(\"dl train: \", type(dl_train))\n",
    "    for (flowpic1, flowpic2, labels) in dl_train:\n",
    "        print(\"flowpic1\", type(flowpic1))\n",
    "        print(\"flowpic2\", type(flowpic2))\n",
    "        print(\"flowpic1 shape: \", flowpic1.shape)\n",
    "        print(\"flowpic2 shape: \", flowpic2.shape)\n",
    "        break    \n",
    "\n",
    "# print batch\n",
    "def print_batch(dl_train) -> None:\n",
    "    for batch in dl_train:\n",
    "        torch.set_printoptions(threshold=sys.maxsize)\n",
    "        print(batch)\n",
    "        break\n",
    "    \n",
    "# debug batch    \n",
    "def debug_batch(dl):\n",
    "    for (flowpic1, flowpic2, labels) in dl:\n",
    "        print(\"flowpic1 type:\", type(flowpic1))\n",
    "        print(\"flowpic2 type:\", type(flowpic2))\n",
    "        # If they are lists, let's check each item\n",
    "        if isinstance(flowpic1, list):\n",
    "            print(f\"flowpic1 has {len(flowpic1)} items.\")\n",
    "            print(\"Shapes of flowpic1 items:\")\n",
    "            for i, fp in enumerate(flowpic1):\n",
    "                print(type(fp))\n",
    "                print(f\"  Item {i}: {fp.shape}\")\n",
    "                break\n",
    "\n",
    "        if isinstance(flowpic2, list):\n",
    "            print(f\"flowpic2 has {len(flowpic2)} items.\")\n",
    "            print(\"Shapes of flowpic2 items:\")\n",
    "            for i, fp in enumerate(flowpic2):\n",
    "                print(f\"  Item {i}: {fp.shape}\")\n",
    "\n",
    "        print(\"labels:\", labels)\n",
    "        break  # Just inspect the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADERS --\n",
    "# TODO: vyresit vyber augmentaci\n",
    "\n",
    "# 0 - debug prints are turned off\n",
    "# 1 - debug prints are turned on\n",
    "DEBUG = 0\n",
    "\n",
    "# remap_label_mirage19 converts label into int\n",
    "# after dp_transform the final tuple is (flowpic tensor, label int)\n",
    "# FIXME: labels nejsou ints\n",
    "dl_train = create_flowpic_dataloader(\n",
    "    #dir_path=\"/workplace/xcocek00/test_a.csv\",\n",
    "    dir_path=\"/workplace/datasets/mirage19/processed/splits/train.csv\",\n",
    "    batch_size=32,\n",
    "    meta_key=\"BF_label\",\n",
    "    time_bins = [i * (300 / 32) for i in range(33)],\n",
    "    length_bins = [i * (1500 / 32) for i in range(33)],\n",
    "    flow_transform_1=packet_loss,\n",
    "    flow_transform_2=packet_loss,\n",
    "   # dp_transform=lambda dp: dp.map(remap_label_mirage19).in_memory_cache(),\n",
    ")\n",
    "dl_val = create_flowpic_dataloader(\n",
    "    dir_path=\"/workplace/datasets/mirage19/processed/splits/val.csv\",\n",
    "    batch_size=32,\n",
    "    meta_key=\"BF_label\",\n",
    "    time_bins = [i * (300 / 32) for i in range(33)],\n",
    "    length_bins = [i * (1500 / 32) for i in range(33)],\n",
    "    flow_transform_1=packet_loss,\n",
    "    flow_transform_2=packet_loss,\n",
    "   # dp_transform=lambda dp: dp.map(remap_label_mirage19).in_memory_cache(),\n",
    ")\n",
    "\n",
    "if DEBUG:    \n",
    "    #print(type(dl_train))\n",
    "    dim_val(dl_train)\n",
    "    #print_batch(dl_train)\n",
    "    #debug_batch(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- simple MLP --\n",
    "# FIXME: DELETE\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, channels, height, width, embed_dim=120):\n",
    "        super().__init__()\n",
    "        input_size = channels * height * width\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, height, width]\n",
    "        x = x.flatten(start_dim=1)  # -> [batch_size, channels*height*width]\n",
    "        z = self.encoder(x)         # -> [batch_size, embed_dim]\n",
    "        z = F.normalize(z, dim=1)   # L2 normalize\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CNN architecture --\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # conv + pooling layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "\n",
    "        # fully connected (fc) layers\n",
    "        self.fc1 = nn.Linear(400, 120) # 16*5*5 = 400 \n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        self.fc3 = nn.Linear(120, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv + ReLU + pool 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # conv + ReLU + pool 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # flatten for fc layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # end of encoder f(·) -> h\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # projection head g(·) -> z\n",
    "        z = F.relu(self.fc2(h))   \n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        # L2 normalize\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        '''\n",
    "        Returns:\n",
    "          h: 120-d representation\n",
    "          z: 30-d projection'\n",
    "        '''\n",
    "        return h, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TEST accuracy calculation --\n",
    "\n",
    "def contrastive_accuracy(z1, z2, temperature=0.07):\n",
    "    '''\n",
    "    Computes contrastive accuracy using a multiclass approach.\n",
    "    \n",
    "    Args:\n",
    "        z1: tensor \n",
    "        z2: tensor \n",
    "        temperature: scaling factor\n",
    "    \n",
    "    '''\n",
    "    batch_size = z1.size(0)\n",
    "    embeddings = torch.cat([z1, z2], dim=0)\n",
    "    \n",
    "    # pairwise cosine similarity scaled by temperature\n",
    "    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "\n",
    "    # mask self-similarity\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=sim_matrix.device)\n",
    "    sim_matrix.masked_fill_(mask, -float(\"inf\"))\n",
    "    \n",
    "    # predictions\n",
    "    preds = sim_matrix.argmax(dim=1)\n",
    "    \n",
    "    # targets\n",
    "    targets = (torch.arange(2 * batch_size, device=sim_matrix.device) + batch_size) % (2 * batch_size)\n",
    "    \n",
    "    acc = FM.accuracy(preds, targets, task=\"multiclass\", num_classes=2 * batch_size)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- training --\n",
    "\n",
    "def train(model, dataloader, optimizer, contrastive_loss_fn, log):\n",
    "    '''\n",
    "    Training loop\n",
    "\n",
    "    Args:\n",
    "        model: chosen model for training\n",
    "        dataloader: pytorch dataloader created by create_flowpic_dataloader function\n",
    "        optimizer: chosen optimizer\n",
    "        contrastive_loss_fn: contrastive loss function (NT-Xent)\n",
    "        log: for log collection\n",
    "\n",
    "    Returns:\n",
    "        Avarage loss \n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batches = 0\n",
    "    acc_total = 0.0\n",
    "\n",
    "    # label not needed in -> _\n",
    "    for flowpic1, flowpic2, _ in dataloader:\n",
    "        flowpic1, flowpic2 = flowpic1.to(device), flowpic2.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass to get embeddings\n",
    "        h1, z1 = model(flowpic1)  \n",
    "        h2, z2 = model(flowpic2)  \n",
    "\n",
    "        # contrastive loss\n",
    "        loss = contrastive_loss_fn(z1, z2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "        \n",
    "        batch_acc = contrastive_accuracy(z1, z2, temperature=0.07)\n",
    "        acc_total += batch_acc.item()\n",
    "\n",
    "    avg_loss = total_loss / batches\n",
    "    avg_acc = acc_total / batches\n",
    "    log[\"loss\"] = avg_loss\n",
    "    log[\"accuracy\"] = avg_acc\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def val(model, dataloader, contrastive_loss_fn, log):\n",
    "    '''\n",
    "    Validation loop\n",
    "\n",
    "    Args:\n",
    "        model: chosen model for training\n",
    "        dataloader: pytorch dataloader created by create_flowpic_dataloader function\n",
    "        contrastive_loss_fn: contrastive loss function (NT-Xent)\n",
    "        log: for log collection\n",
    "\n",
    "    Returns:\n",
    "        Avarage loss \n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        batches = 0\n",
    "        acc_total = 0.0\n",
    "\n",
    "        # label not needed in -> _\n",
    "        for flowpic1, flowpic2, _ in dataloader:\n",
    "            flowpic1, flowpic2 = flowpic1.to(device), flowpic2.to(device)\n",
    "\n",
    "            # forward pass to get embeddings\n",
    "            h1, z1 = model(flowpic1)  \n",
    "            h2, z2 = model(flowpic2)  \n",
    "\n",
    "            # contrastive loss\n",
    "            loss = contrastive_loss_fn(z1, z2)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batches += 1\n",
    "            \n",
    "            batch_acc = contrastive_accuracy(z1, z2, temperature=0.07)\n",
    "            acc_total += batch_acc.item()\n",
    "\n",
    "        avg_loss = total_loss / batches\n",
    "        avg_acc = acc_total / batches\n",
    "        log[\"val_loss\"] = avg_loss\n",
    "        log[\"val_accuracy\"] = avg_acc\n",
    "\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- loss function --\n",
    "\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    '''\n",
    "    NT-Xent (Normalized Temperature-Scaled Cross-Entropy) loss function\n",
    "    \n",
    "    Args:\n",
    "        z1: tensor \n",
    "        z2: tensor \n",
    "        temperature: scaling factor\n",
    "    \n",
    "    Returns:\n",
    "        A scalar loss value\n",
    "    '''\n",
    "    \n",
    "    batch_size = z1.shape[0]\n",
    "    \n",
    "    # concatenate embeddings -> shape [2N, embed_dim]\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    \n",
    "    # cosine similarity matrix scaled by temperature\n",
    "    sim_matrix = torch.matmul(z, z.T) / temperature\n",
    "    \n",
    "    # mask to remove self-similarities (diagonal entries)\n",
    "    diag_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "    sim_matrix.masked_fill_(diag_mask, -float('inf'))\n",
    "    \n",
    "    # for each i the positive example is:\n",
    "    # - i in [0, N-1] then positive is: i + N\n",
    "    # - i in [N, 2N-1] then positive is: i - N\n",
    "    positives = torch.cat([\n",
    "        torch.arange(batch_size, 2 * batch_size),\n",
    "        torch.arange(0, batch_size)\n",
    "    ], dim=0).to(z.device)\n",
    "    \n",
    "    # cross entropy loss\n",
    "    loss = F.cross_entropy(sim_matrix, positives)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirage19_classes = 20\n",
    "\n",
    "# model\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# NT-Xent loss function\n",
    "loss_fn = nt_xent_loss\n",
    "\n",
    "liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6, 2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    log = {}\n",
    "    train_loss = train(model, dl_train, optimizer, loss_fn, log)\n",
    "    val_loss = val(model, dl_val, loss_fn, log)\n",
    "\n",
    "    liveloss.update(log)\n",
    "    liveloss.send()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
